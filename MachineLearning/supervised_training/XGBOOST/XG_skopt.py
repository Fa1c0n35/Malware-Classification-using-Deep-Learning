
import json
import re
import copy
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from skopt import BayesSearchCV
from skopt.space import Real, Integer
from sklearn.model_selection import StratifiedKFold
import xgboost as xgb
import pandas as pd
import numpy as np


def main():

    df_train = pd.read_csv('../train_dataset.csv')
    df_test = pd.read_csv('../test_dataset.csv')

    X_train, y_train = df_train.iloc[:, 2:].values, df_train.iloc[:, 0].values
    X_test, y_test = df_test.iloc[:, 2:].values, df_test.iloc[:, 0].values

    # log-uniform: understand as search over p = exp(x) by varying x
    opt = BayesSearchCV(
        estimator=xgb.XGBClassifier(),

        # ref: https://github.com/automl/auto-sklearn/blob/master/autosklearn/pipeline/components/classification/xgradient_boosting.py
        search_spaces={
            'learning_rate': Real(0.001, 1.0, 'log-uniform'),
            'min_child_weight': Integer(0, 20),
            'max_depth': Integer(0, 50),
            'max_delta_step': Integer(0, 20),
            'subsample': Real(0.01, 1.0, 'uniform'),
            'colsample_bytree': Real(0.01, 1.0, 'uniform'),
            'colsample_bylevel': Real(0.01, 1.0, 'uniform'),
            'reg_lambda': Real(1e-10, 1e-1, 'log-uniform'),
            'reg_alpha': Real(1e-10, 1e-1, 'log-uniform'),
            'gamma': Real(1e-9, 0.5, 'log-uniform'),
            'n_estimators': Integer(50, 512),
            'scale_pos_weight': Real(1e-6, 500, 'log-uniform'),
            'booster': ["gbtree", "dart"],
            'sample_type': ['uniform', 'weighted'],
            'normalize_type': ['tree', 'forest'],
            'rate_drop': Real(1e-10, 1-(1e-10), 'uniform')
        },
        cv = StratifiedKFold(
            n_splits=10,
            shuffle=True
        ),
        n_jobs=3,
        n_iter=100,
        verbose=0,
        refit=True,
        random_state=42
    )

    def status_print(_):
        """Status callback durring bayesian hyperparameter search"""

        # Get all the models tested so far in DataFrame format
        all_models = pd.DataFrame(opt.cv_results_)

        best_parap_copy = copy.deepcopy(opt.best_params_)
        for k, v in opt.best_params_.items():
            best_parap_copy[k] = v if isinstance(
                v, str) or isinstance(v, float) else v.item()
        param_list = []
        for each in json.dumps(best_parap_copy)[1:-1].split(', '):
            param_list.append('='.join(each[1:].split('": ')))

        if hasattr(opt.estimator, 'verbose'):
            param_list.append('verbose=True')

        param = opt.estimator.__class__.__name__ + \
            '(' + ', '.join(param_list) + ')'

        # Get current parameters and the best parameters
        print('Model #{}\nBest roc_auc: {}\nBest params: {}\n'.format(
            len(all_models),
            np.round(opt.best_score_, 4),
            param
        ))

    opt.fit(X_train, y_train, callback=status_print)

    print("val. score: %s" % opt.best_score_)
    print("test score: %s" % opt.score(X_test, y_test))


if __name__ == '__main__':
    main()
