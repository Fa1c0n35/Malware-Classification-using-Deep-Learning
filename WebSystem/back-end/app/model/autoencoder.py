
import torch


class AutoEncoder(torch.nn.Module):

    def __init__(self, layer_size):
        """
        Initialize the AutoEncoder class
        """

        # must have at least [input, hidden, output] layers
        assert len(layer_size) >= 3
        assert layer_size[0] == layer_size[-1]  # input equals output
        assert len(layer_size) % 2 == 1  # must have odd number of layers

        # initialize nn.Module object
        super(AutoEncoder, self).__init__()

        # save parameters
        self.layer_size = layer_size

        # prepare func locally
        self.relu = torch.nn.ReLU()

        self.layers = torch.nn.ModuleList()
        self.batchnorm = torch.nn.ModuleList()
        for i in range(len(self.layer_size)-1):
            self.layers.append(
                torch.nn.Linear(self.layer_size[i], self.layer_size[i+1]))
            if i < len(self.layer_size)-2:
                self.batchnorm.append(
                    torch.nn.BatchNorm1d(self.layer_size[i+1]))

    def forward(self, x):

        # hidden layer
        encoded = None
        for i, (layer, batchnorm) in enumerate(zip(self.layers[:-1], self.batchnorm)):
            x = layer(x)
            # can only be applied for NxM, where N is batch size & M = data size
            x = batchnorm(x)
            x = self.relu(x)
            if i == len(self.layer_size)//2-1:  # get middle (thus encoded data)
                encoded = x

        decoded = self.layers[-1](x)

        return encoded, decoded