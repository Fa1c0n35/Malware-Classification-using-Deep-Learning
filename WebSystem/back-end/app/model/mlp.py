
import torch


class MultilayerPerceptron(torch.nn.Module):

    def __init__(self, layer_size):
        """
        Initialize the MultilayerPerceptron class
        """

        # initialize nn.Module object
        super(MultilayerPerceptron, self).__init__()

        # save parameters
        self.layer_size = layer_size

        # prepare func locally
        self.relu = torch.nn.ReLU()
        self.batchnorm = torch.nn.ModuleList()

        self.layers = torch.nn.ModuleList()
        for i in range(len(self.layer_size)-1):
            self.layers.append(
                torch.nn.Linear(self.layer_size[i], self.layer_size[i+1]))
            if i < len(self.layer_size)-2:
                self.batchnorm.append(
                    torch.nn.BatchNorm1d(self.layer_size[i+1]))

    def forward(self, x):

        # hidden layer
        for layer, batchnorm in zip(self.layers[:-1], self.batchnorm):
            x = layer(x)
            # can only be applied for NxM, where N=batch size and M=data size
            x = batchnorm(x)
            x = self.relu(x)

        # output layer
        x = self.layers[-1](x)

        return x